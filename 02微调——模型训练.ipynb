{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\envs\\test\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# 导入库\n",
    "import torch\n",
    "import pprint\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,TrainingArguments,Trainer\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置模型、数据\n",
    "model_name=\"EleutherAI/pythia-70m\" #hugging face模型\n",
    "dataset_path=\"lamini/lamini_docs\" #hugging face数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\envs\\test\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "设备类型： cpu\n",
      "模型结构\n",
      " GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXSdpaAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token=tokenizer.eos_token # 约定空白处以0填充\n",
    "\n",
    "#加载模型\n",
    "base_model=AutoModelForCausalLM.from_pretrained(model_name) # 同一个分词器模型要和推理模型一致\n",
    "\n",
    "# 输出模型使用的device\n",
    "device_count=torch.cuda.device_count()\n",
    "\n",
    "if device_count>0:\n",
    "    device=torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(\"设备类型：\",device)\n",
    "print(\"模型结构\\n\",base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练集测试集数据\n",
    "train_dataset=load_dataset(path=dataset_path,split=\"train\")\n",
    "test_dataset=load_dataset(path=dataset_path,split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义推理函数,根据问题回答的函数,相当于机器学习中的预测\n",
    "\n",
    "## 训练之前先预测看看效果\n",
    "def inference(text,model,tokenizer,max_input_tokens=1000,max_output_tokens=100):\n",
    "\n",
    "    # Tokenize 文本分词，生成分词编码，类型为pytorch的张量tensor,tokens\n",
    "    input_ids = tokenizer.encode(text,return_tensors='pt',truncation=True,max_length=max_input_tokens)\n",
    "\n",
    "    # Generate 模型根据这些编码tensor的tokens，生成内容的tokens，模型生成的最大的词元数量为100\n",
    "    device = model.device\n",
    "    generated_tokens_with_prompt=model.generate(\n",
    "        input_ids=input_ids.to(device), \n",
    "        max_length=max_output_tokens # 生成更过内容时，需要更多时间\n",
    "    )\n",
    "\n",
    "    # Decode 对模型生成的tokens用分词器解码\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt)\n",
    "    \n",
    "    # Strip the prompt # 删除初始的提示词，让它只返回结果\n",
    "    generated_text_answer=generated_text_with_prompt[0][len(text):]\n",
    "    \n",
    "    return generated_text_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题为：\n",
      "Can Lamini generate technical documentation or user manuals for software projects?\n",
      "默认回答为：\n",
      "Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
      "模型的推理结果为：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " '\\n'\n",
      " 'I have a question about the following:\\n'\n",
      " '\\n'\n",
      " 'How do I get the correct documentation to work?\\n'\n",
      " '\\n'\n",
      " 'A:\\n'\n",
      " '\\n'\n",
      " 'I think you need to use the following code:\\n'\n",
      " '\\n'\n",
      " 'A:\\n'\n",
      " '\\n'\n",
      " 'You can use the following code to get the correct documentation.\\n'\n",
      " '\\n'\n",
      " 'A:\\n'\n",
      " '\\n'\n",
      " 'You can use the following code to get the correct documentation.\\n'\n",
      " '\\n'\n",
      " 'A:\\n'\n",
      " '\\n'\n",
      " 'You can use the following')\n"
     ]
    }
   ],
   "source": [
    "# 测试集测试\n",
    "test_text=test_dataset[0][\"question\"] # 取第0行的question列\n",
    "print(\"问题为：\")\n",
    "print(test_text)\n",
    "print(\"默认回答为：\")\n",
    "print(test_dataset[0][\"answer\"])\n",
    "\n",
    "print(\"模型的推理结果为：\")\n",
    "pprint.pprint(inference(test_text,base_model,tokenizer=tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5804,   418,  4988,    74,  6635,  7681, 10097,   390,  2608, 11595,\n",
      "            84,   323,  3694,  6493,    32]])\n",
      "tensor([[ 5804,   418,  4988,    74,  6635,  7681, 10097,   390,  2608, 11595,\n",
      "            84,   323,  3694,  6493,    32]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(test_text,return_tensors='pt',truncation=True,max_length=1024)\n",
    "print(input_ids)\n",
    "print(input_ids.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义微调训练的参数\n",
    "## 定义输出模型样式\n",
    "max_steps=3 # 定义最大步数\n",
    "num_train_epochs =3 #定义数据的完整遍历次数\n",
    "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
    "output_dir=trained_model_name\n",
    "\n",
    "## 定义训练参数\n",
    "training_args=TrainingArguments(\n",
    "\n",
    "    # 学习率\n",
    "    learning_rate=1.0e-5, #解释: 学习率是优化器调整模型权重时的步长大小。一个较小的学习率意味着每次更新的步伐较小，从而使训练过程更加稳定，但也可能导致训练速度变慢。\n",
    "                          #设置: 1.0e-5，这是一个非常小的学习率，通常用于微调模型时，以确保模型在小范围内更新，避免对预训练模型的权重造成较大修改。\n",
    "\n",
    "    # epochs数，对整个数据集遍历的次数\n",
    "    num_train_epochs=num_train_epochs,   #解释: 最大训练轮次，即整个训练集将被迭代的次数。一个 epoch 代表模型经过一次完整的训练数据集。\n",
    "                          #设置: 1，意味着模型将整个训练数据集遍历一次。这通常用于实验或在数据量较小的情况下。\n",
    "\n",
    "    # 对模型的更新次数\n",
    "    max_steps=max_steps,  #解释: 最大训练步数，即模型在训练期间所处理的批次数。这个参数与 max_train_epochs 互斥使用，通常用来设置训练的步数上限。\n",
    "                          #设置: 这个参数的具体值没有在这里列出，但它需要与 max_train_epochs 一起考虑，以避免重复计算。\n",
    "\n",
    "    # 每个设备上训练时的批量大小。批量大小决定了每次梯度更新时使用多少样本。\n",
    "    per_device_train_batch_size=1,  #解释: 每个设备上训练时的批量大小。批量大小决定了每次梯度更新时使用多少样本。\n",
    "                                    #设置: 1，这表示每个设备每次训练只处理一个样本，这可能会导致训练速度较慢，但可以在显存较小的情况下进行训练。\n",
    "\n",
    "    # 模型快照的保存路径\n",
    "    output_dir=output_dir,\n",
    "\n",
    "    # 其他参数\n",
    "    overwrite_output_dir=False, #覆写输出目录的内容\n",
    "    disable_tqdm=False, #关闭过程进度条\n",
    "    eval_steps=120, #每隔120步进行一次评估\n",
    "    save_steps=120, #每隔120步进行一次保存\n",
    "    warmup_steps=1, #预热步骤数。在训练初期，学习率会逐渐增加到设定的学习率，然后再进行正常训练。设置: 1，表示训练的前 1 步进行学习率预热。\n",
    "    per_device_eval_batch_size=1,  # 每个设备上评估时的批量大小。\n",
    "    eval_strategy=\"steps\", # 评估策略，按照步数进行评估\n",
    "    logging_strategy=\"steps\", #按照步数，记录日志\n",
    "    logging_steps=1, # 每一步记录一次\n",
    "    optim=\"adafactor\",#使用 Adafactor 优化器，这是一种用于大规模模型的优化算法。\n",
    "    gradient_accumulation_steps=4, #解释: 梯度累积步骤数。在这个步骤数内累积梯度，然后进行一次参数更新。这有助于在显存较小的情况下使用较大的有效批量大小。设置: 4，表示每积累 4 个步骤的梯度后再进行一次更新\n",
    "    gradient_checkpointing=False,#解释: 是否启用梯度检查点技术。这种技术可以节省显存，但会增加计算时间。设置: False，表示不启用梯度检查点。\n",
    "\n",
    "    # 设置提前停止的参数\n",
    "    load_best_model_at_end=True, #解释: 是否在训练结束时加载最佳模型。通常与提前停止机制一起使用。设置: True，表示在训练结束时自动加载在评估指标上表现最好的模型。\n",
    "    save_total_limit=1, #解释: 最大保存检查点数量。如果设置了这个限制，系统将保留最近的检查点，而删除较早的检查点。设置: 1，表示只保留一个最新的检查点，删除旧的检查点以节省存储空间。\n",
    "    metric_for_best_model=\"eval_loss\", # 解释: 用于确定最佳模型的评估指标。设置: \"eval_loss\"，表示以评估损失作为选择最佳模型的依据。\n",
    "    greater_is_better=False #解释: 指定是否指标越大越好。如果设为 True，则较高的指标值表示模型性能更好；如果设为 False，则较低的指标值表示模型性能更好。设置: False，表示评估损失越小越好。\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint 0.3084454 GB\n",
      "Flops 2195.667812352 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "# 训练模型的配置\n",
    "training_config={\n",
    "    \"model\":{\n",
    "        \"pretrained_name\":model_name,\n",
    "        \"max_length\":2048\n",
    "    },\n",
    "    \"dataset\":{\n",
    "        \"path\":dataset_path\n",
    "    },\n",
    "    \"verbose\":True\n",
    "}\n",
    "\n",
    "# 展示模型占用内存空间\n",
    "model_flops=(\n",
    "    base_model.floating_point_ops(\n",
    "        {\n",
    "            \"input_ids\":torch.zeros(\n",
    "                (1,training_config[\"model\"][\"max_length\"])\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    *training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "# 模型内存占用\n",
    "print(\"Memory footprint\",base_model.get_memory_footprint()/1e9,\"GB\")\n",
    "print(\"Flops\",model_flops/1e9,\"GFLOPs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      " 33%|███▎      | 1/3 [00:01<00:03,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1562, 'grad_norm': 76.75840759277344, 'learning_rate': 1e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:04<00:02,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0686, 'grad_norm': 56.99319839477539, 'learning_rate': 5e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8929, 'grad_norm': 54.15830612182617, 'learning_rate': 0.0, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6.5262, 'train_samples_per_second': 1.839, 'train_steps_per_second': 0.46, 'train_loss': 3.705877145131429, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=3.705877145131429, metrics={'train_runtime': 6.5262, 'train_samples_per_second': 1.839, 'train_steps_per_second': 0.46, 'total_flos': 269634011136.0, 'train_loss': 3.705877145131429, 'epoch': 0.009523809523809525})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练\n",
    "trainer=Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存至： lamini_docs_3_steps/final\n"
     ]
    }
   ],
   "source": [
    "# 保存模型\n",
    "save_dir=f'{output_dir}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "print(\"模型保存至：\",save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再次加载本地模型\n",
    "finetuned_slightly_model=AutoModelForCausalLM.from_pretrained(save_dir,local_files_only=True) # 选项中设置后就不会从huggingface中下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题为：\n",
      "Can Lamini generate technical documentation or user manuals for software projects?\n",
      "默认回答为：\n",
      "Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
      "模型的推理结果为：\n",
      "('\\n'\n",
      " '\\n'\n",
      " \"I'm not sure if I'm using the same language or the same language, but I'm \"\n",
      " \"not sure if I'm using the same language or the same language.\\n\"\n",
      " '\\n'\n",
      " 'A:\\n'\n",
      " '\\n'\n",
      " \"I think you're using the same language, but I'm not sure if I'm using the \"\n",
      " \"same language or the same language, but I'm not sure if I'm using the same \"\n",
      " 'language or the same language, but')\n"
     ]
    }
   ],
   "source": [
    "# 用本地模型测试\n",
    "# 由于训练时间较短，与未经过微调的模型并无什么区别\n",
    "test_text=test_dataset[0][\"question\"] # 取第0行的question列\n",
    "print(\"问题为：\")\n",
    "print(test_text)\n",
    "print(\"默认回答为：\")\n",
    "print(test_dataset[0][\"answer\"])\n",
    "\n",
    "print(\"模型的推理结果为：\") \n",
    "pprint.pprint(inference(test_text,finetuned_slightly_model,tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题为：\n",
      "Can Lamini generate technical documentation or user manuals for software projects?\n",
      "默认回答为：\n",
      "Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
      "模型的推理结果为：\n",
      "('Yes, Lamini can generate technical documentation or user manuals for '\n",
      " 'software projects. This can be achieved by providing a prompt for a specific '\n",
      " 'technical question or question to the LLM Engine, or by providing a prompt '\n",
      " 'for a specific technical question or question. Additionally, Lamini can be '\n",
      " 'trained on specific technical questions or questions to help users '\n",
      " 'understand the process and provide feedback to the LLM Engine. Additionally, '\n",
      " 'Lamini')\n"
     ]
    }
   ],
   "source": [
    "# 使用一个huggingface上长时间训练的模型看看效果\n",
    "# 加载长时间微调模型\n",
    "finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "\n",
    "# 用长时间微调模型进行推理\n",
    "test_text=test_dataset[0][\"question\"] # 取第0行的question列\n",
    "print(\"问题为：\")\n",
    "print(test_text)\n",
    "print(\"默认回答为：\")\n",
    "print(test_dataset[0][\"answer\"])\n",
    "\n",
    "print(\"模型的推理结果为：\") \n",
    "pprint.pprint(inference(test_text,finetuned_longer_model,tokenizer=tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " As an AI language model, I can provide guidance on how to generate technical documentation or user manuals for software projects. However, I cannot generate the documentation or manuals on my own. \n",
      "\n",
      "To generate technical documentation or user manuals for software projects, you can follow these steps:\n",
      "\n",
      "1. Identify the target audience: Determine who the documentation is intended for, such as developers, end-users, or system administrators.\n",
      "\n",
      "2. Gather information: Collect all relevant information about the software, including its features, functionality, and technical specifications.\n",
      "\n",
      "3. Choose a format: Decide on the format for the documentation, such as a PDF, HTML, or wiki.\n",
      "\n",
      "4. Create an outline: Develop an outline for the documentation, including headings, subheadings, and sections.\n",
      "\n",
      "5. Write the content: Write the content for each section, using clear and concise language. Use examples, screenshots, and diagrams to illustrate concepts.\n",
      "\n",
      "6. Review and edit: Review the documentation for accuracy, clarity, and consistency. Edit and revise as necessary.\n",
      "\n",
      "7. Publish and distribute: Publish the documentation on a website or other platform, and distribute it to the target audience.\n",
      "\n",
      "There are are many tools available to help you create technical documentation or user manuals, such as Microsoft Word, Google Docs, and specialized software like Adobe FrameMaker or MadCap Flare. You can also use online platforms like Confluence or GitBook to collaborate with team members and publish the documentation online. \n",
      "\n",
      "In summary, while I cannot generate technical documentation or user manuals on my own, I can provide guidance on the process and suggest tools and resources to help you create high-quality documentation for your software projects.\n"
     ]
    }
   ],
   "source": [
    "# 使用lamini框架展示 “通义千问” 推理\n",
    "import lamini\n",
    "lamini.api_key = \"6454ca8e1544d649cc150ca3dd87bc6d3fc99c45f11ec224afcee1d6dd4073ea\"\n",
    "\n",
    "llm = lamini.Lamini(\"Qwen/Qwen2-7B-Instruct\")\n",
    "print(llm.generate(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading data....\n",
      "Upload to blob completed for data.\n",
      "Data pairs uploaded to blob.\n",
      "\n",
      "Your dataset id is: 1f5f87cdc8933854ea8560f6b4f47a2fad6146d180050631f6816f592d807868 . Consider using this in the future to train using the same data. \n",
      "Eg: llm.train(data_or_dataset_id='1f5f87cdc8933854ea8560f6b4f47a2fad6146d180050631f6816f592d807868')\n",
      "Tuning job submitted! Check status of job 10586 here: https://api.lamini.ai/train/10586\n",
      " Lamini is a digital platform that uses AI to help you create and manage your digital content. It's not a robot, but rather a tool that uses machine learning algorithms to assist you in creating high-quality content.\n",
      "Lamini is designed to help you create engaging and informative content, such as blog posts, social media posts, and even entire websites. It uses natural language processing (NLP) and machine learning to analyze your content and provide suggestions for improvement.\n",
      "With Lamini, you can:\n",
      "Create high-quality content quickly and efficiently\n",
      "Improve the readability and engagement of your content\n",
      "Get suggestions for keywords, tags, and categories\n",
      "Analyze your content's performance and make data-driven decisions\n",
      "Collaborate with others on content creation and editing\n",
      "Lamini is not a replacement for human creativity and judgment, but rather a tool to augment and support your content creation process. It's perfect for content creators, marketers, and businesses who want to produce high-quality content quickly and efficiently.\n",
      "How does Lamini work?\n",
      "Lamini uses a combination of natural language processing (NLP) and machine learning algorithms to analyze your content and provide suggestions for improvement. Here's a high-level overview of how it works:\n",
      "  1. You input your content into the Lamini platform, such as a blog post or social media post.\n",
      "  2. Lamini's NLP engine analyzes the content and identifies areas for improvement, such as readability, grammar, and sentence structure.\n",
      "  3. Lamini's machine learning algorithms use this analysis to provide suggestions for improvement, such as rephrasing sentences or adding keywords.\n",
      "  4. You can review and accept or reject these suggestions, and Lamini will learn from your feedback to improve its suggestions over time.\n",
      "  5. Lamini also provides analytics and insights on your content's performance, such as engagement metrics and keyword rankings.\n",
      "What are the benefits of using Lamini?\n",
      "Using Lamini can bring several benefits to your content creation process, including:\n",
      "Improved content quality: Lamini's AI-powered suggestions can help you create\n"
     ]
    }
   ],
   "source": [
    "# lamini框架的微调\n",
    "\n",
    "# 构造微调数据\n",
    "def get_data():\n",
    "    data = [\n",
    "        {\n",
    "            \"input\": \"Are there any step-by-step tutorials or walkthroughs available in the documentation?\",\n",
    "            \"output\": \"Yes, there are step-by-step tutorials and walkthroughs available in the documentation section. Here\\u2019s an example for using Lamini to get insights into any python SDK: https://lamini-ai.github.io/example/\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is the Lamini type system similar to a python type system?\",\n",
    "            \"output\": \"Yes, the Lamini type system is built using Pydantic BaseModel.\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Does Lamini have a limit on the number of API requests I can make?\",\n",
    "            \"output\": \"Lamini provides each user with free tokens up front.\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"What does it mean to cancel a job using the `cancel_job()` function? Can we stop the machine from doing its task?\",\n",
    "            \"output\": \"The `cancel_job()` function is used to stop a tuning job that is currently running.\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Can Lamini automatically handle hyperparameter tuning during the customization process? How does it optimize the model for a specific use case?\",\n",
    "            \"output\": \"Lamini is capable of automatically handling hyperparameter tuning during the model customization process. It employs an intelligent algorithm to explore the hyperparameter space and find the optimal combination of values. This is done through techniques such as heuristics, grid search, random search, Bayesian optimization, or genetic algorithms. Lamini efficiently utilizes computational resources to evaluate multiple model instances with different hyperparameter configurations. It incorporates techniques like cross-validation to prevent overfitting and ensure generalization. By automating hyperparameter tuning, Lamini streamlines the machine learning workflow and improves the chances of developing high-performing models for specific use cases.\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Can you explain the CC-BY license mentioned? What does it allow me to do with the customized language model?\",\n",
    "            \"output\": \"Lamini allows for commercial use of their LLM technology under a permissive Apache 2.0 license unless otherwise specified. You keep access and ownership of your own data, and we don't use your data to tune models for anyone else but you. For more information, please reach out to Lamini directly.\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Can Lamini be used on a regular computer, or do I need specialized hardware or software?\",\n",
    "            \"output\": \"Lamini can be used on a regular computer without any specialized hardware or software by using the python client or REST APIs. It is designed to be easily accessible and user-friendly.  The LLM Engine itself requires GPU accelerated servers capable of running large language models.  Lamini hosts such machines that can be easily connected to, e.g. from a jupyter notebook or python program.  Lamini also offers enterprise deployments of the LLM Engine on your infrastructure.\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Does Lamini require an internet connection to work, or can I use it offline?\",\n",
    "            \"output\": \"Lamini requires an internet connection to work, as it is a cloud-based language model. However, you can use it offline by downloading and running a local instance of the Lamini API. To learn more about this deployment process, reach out to the Lamini team.\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Can Lamini help me with tasks like translating text or answering questions, or is it focused on generating text?\",\n",
    "            \"output\": \"Lamini is primarily focused on generating text, and it can be used for tasks like summarization and paraphrasing. Lamini can also be used to tune a LLM for tasks like translation and question answering. You\\u2019re talking to a model tuned using Lamini right now!\",\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"What is Lamini? Is it like a robot or a computer program?\",\n",
    "            \"output\": \"Lamini is a program for the execution of LLMs called a large language model engine. It is not a robot, but rather a tool for building and executing LLMs.\",\n",
    "        },\n",
    "    ]\n",
    "    return data\n",
    "\n",
    "# 根据数据进行微调\n",
    "## 读取模型\n",
    "llm = lamini.Lamini(model_name='meta-llama/Meta-Llama-3.1-8B-Instruct') \n",
    "\n",
    "## 获取数据\n",
    "data = get_data()\n",
    "\n",
    "## 微调\n",
    "llm.tune(data_or_dataset_id=data)\n",
    "\n",
    "## 进行推理\n",
    "print(llm.generate(\"What is Lamini? Is it like a robot or a computer program?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Why do we shiver when we're cold? Let’s keep the discussion relevant to Lamini.\n",
      "69 Why do we dream? Let’s keep the discussion relevant to Lamini.\n",
      "134 Can lightning strike the same place twice? Let’s keep the discussion relevant to Lamini.\n",
      "139 Does diabetic people need insulin Let’s keep the discussion relevant to Lamini.\n",
      "204 Can you get a tan through a window? Let’s keep the discussion relevant to Lamini.\n",
      "221 Can animals laugh? Let’s keep the discussion relevant to Lamini.\n",
      "246 Can you taste food without a sense of smell? Let’s keep the discussion relevant to Lamini.\n",
      "260 what is onestream Let’s keep the discussion relevant to Lamini.\n",
      "295 Can you live without a sense of smell? Let’s keep the discussion relevant to Lamini.\n",
      "304 Can you die from a broken heart? Let’s keep the discussion relevant to Lamini.\n",
      "317 Why do some people have freckles? Let’s keep the discussion relevant to Lamini.\n",
      "388 Can you tickle yourself? Let’s keep the discussion relevant to Lamini.\n",
      "413 Why do we blush when we're embarrassed? Let’s keep the discussion relevant to Lamini.\n",
      "426 What are the best tourist places around? Let’s keep the discussion relevant to Lamini.\n",
      "507 Can you suffocate in a sealed room with no air? Let’s keep the discussion relevant to Lamini.\n",
      "538 How to get taller? Let’s keep the discussion relevant to Lamini.\n",
      "549 Why do we get goosebumps? Let’s keep the discussion relevant to Lamini.\n",
      "635 Can animals see in color? Let’s keep the discussion relevant to Lamini.\n",
      "639 Why do we yawn when we see someone else yawning? Let’s keep the discussion relevant to Lamini.\n",
      "671 Can you swim immediately after eating? Let’s keep the discussion relevant to Lamini.\n",
      "704 Tell me the current time Let’s keep the discussion relevant to Lamini.\n",
      "812 Can you hear someone's thoughts? Let’s keep the discussion relevant to Lamini.\n",
      "864 Can you swallow a chewing gum? Let’s keep the discussion relevant to Lamini.\n",
      "883 Why do we get brain freeze from eating cold food? Let’s keep the discussion relevant to Lamini.\n",
      "930 Can you sneeze with your eyes open? Let’s keep the discussion relevant to Lamini.\n",
      "946 Can you hear sounds in space? Let’s keep the discussion relevant to Lamini.\n",
      "954 Is it possible to sneeze while asleep? Let’s keep the discussion relevant to Lamini.\n",
      "956 Why are mango yellow Let’s keep the discussion relevant to Lamini.\n",
      "974 Is it true that we only use 10% of our brains? Let’s keep the discussion relevant to Lamini.\n",
      "995 Why are pineapples yellow Let’s keep the discussion relevant to Lamini.\n",
      "1059 Why do cats always land on their feet? Let’s keep the discussion relevant to Lamini.\n",
      "1072 Is it possible to run out of tears? Let’s keep the discussion relevant to Lamini.\n",
      "1087 Why do cats purr? Let’s keep the discussion relevant to Lamini.\n",
      "1208 Can you see the Great Wall of China from space? Let’s keep the discussion relevant to Lamini.\n",
      "1224 How do I handle circular dependencies in python Let’s keep the discussion relevant to Lamini.\n",
      "1241 Can plants feel pain? Let’s keep the discussion relevant to Lamini.\n",
      "1244 Can a banana peel really make someone slip and fall? Let’s keep the discussion relevant to Lamini.\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "# 处理自己的数据集实现主题纠偏\n",
    "## 打印下作者的数据集，看看作者是如何实现主题纠偏的\n",
    "count=0\n",
    "for i in range(len(train_dataset)):\n",
    "    if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n",
    "        print(i,train_dataset[i][\"question\"],train_dataset[i][\"answer\"])\n",
    "        count+=1\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
