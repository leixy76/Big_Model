{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "156264d7-30e1-4c44-97a7-04d12a80eeec",
   "metadata": {},
   "source": [
    "## 任务4：文本多路召回与重排序\n",
    "## BM25+BGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa99e185-cce1-4ebb-87e1-c78b698f65f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据库\n",
    "import jieba,json,pdfplumber\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb15c06-e722-459a-83d3-438acaf40487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练数据\n",
    "\n",
    "## 加载问题\n",
    "questions = json.load(open(\"../data/Coggle比赛数据/汽车知识问答/questions.json\",encoding=\"utf8\"))\n",
    "\n",
    "## 加载知识pdf\n",
    "pdf = pdfplumber.open(\"../data/Coggle比赛数据/汽车知识问答/初赛训练数据集.pdf\")\n",
    "\n",
    "# 预处理,把每一页的页码和一整页的内容拼凑在一个json当中，且添加到list\n",
    "pdf_content = []\n",
    "for page_idx in range(len(pdf.pages)):\n",
    "    pdf_content.append({\n",
    "        \"page\": \"page_\"+str(page_idx+1), # 严格一一对应，第几页就是第几页的内容，\n",
    "        \"content\": pdf.pages[page_idx].extract_text()\n",
    "    })\n",
    "\n",
    "pdf_content[-99] # 最后默认有一个页码，数的是256页，对应内容的页码也是256页，严格一一对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15cab23d-f3ff-4208-b213-9b75b4342988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25处理数据\n",
    "\n",
    "## jieba：把每一页内容切分成单词,放在一个列表里，也就是大列表中包含354个小列表，每个小列表中包含很多个单词\n",
    "pdf_content_words = [jieba.lcut(x[\"content\"]) for x in pdf_content] # \n",
    "bm25 = BM25Okapi(pdf_content_words) # 喂给bm25的就是单词\n",
    "\n",
    "\n",
    "# 结巴会把拆分的词放在一个列表\n",
    "test = \"你好我是你的爸爸我热爱打你\"\n",
    "result = jieba.lcut(test)\n",
    "# result : ['你好', '我', '是', '你', '的', '爸爸', '我', '热爱', '打', '你']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf63099-fbb7-4288-b729-d7ba14866c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 模型处理数据\n",
    "\n",
    "## 加载模型\n",
    "model = SentenceTransformer(\"BAAI/bge-small-zh-v1.5\",device='cuda')\n",
    "\n",
    "## 所有的问题放在一个列表，所有的内容也放在一个列表\n",
    "questions_sentences = [x['question'] for x in questions] # 只有一个列表，列表中的一项就是一个问题\n",
    "pdf_content_sentences = [x[\"content\"] for x in pdf_content] # 只有一个列表，列表中的一项就是一页内容\n",
    "\n",
    "# # embedding处理\n",
    "question_embeddings = model.encode(questions_sentences,normalize_embeddings=True)\n",
    "pdf_embeddings = model.encode(pdf_content_sentences,normalize_embeddings=True)\n",
    "\n",
    "pdf_content_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5089d3d-02a6-4167-8d18-a0d05d509a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 双路召回\n",
    "from scipy.stats import rankdata #用于为数组中的元素分配排名，\n",
    "for query_idx,feat in enumerate(question_embeddings):\n",
    "\n",
    "    # embedding计算相似度并打分\n",
    "    score1 = feat@pdf_embeddings.T \n",
    "    score2 = bm25.get_scores(jieba.lcut(questions[query_idx]['question'])) # 合乎处理\n",
    "    \n",
    "    score = rankdata(score1)+rankdata(score2)\n",
    "    max_score_page_idx = score.argsort()[-1] + 1\n",
    "    questions[query_idx][\"reference\"] = 'page_' + str(max_score_page_idx)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87b19ad6-6a78-4cdf-b63a-2bab242796a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存双路召回的结果\n",
    "with open('../data/Coggle比赛数据/汽车知识问答/submit_bm25_bgesmall.json', 'w', encoding='utf8') as up:\n",
    "    json.dump(questions, up, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8ce4a-1cea-451a-bc7a-d9a21f8537bf",
   "metadata": {},
   "source": [
    "## BGE/Text Segment(语义模型配合文本分词)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0f58cd8-0362-427f-8178-791a8f5376bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库\n",
    "import jieba, json, pdfplumber\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "## 加载问题\n",
    "questions = json.load(open(\"../data/Coggle比赛数据/汽车知识问答/questions.json\",encoding=\"utf8\"))\n",
    "\n",
    "## 加载知识pdf\n",
    "pdf = pdfplumber.open(\"../data/Coggle比赛数据/汽车知识问答/初赛训练数据集.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "320e1c47-fe30-4583-bd8f-b10b6901c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分文本\n",
    "def split_text_fixed_size(text,chunk_size): # 拆分文本\n",
    "    _ = [text[i : i+chunk_size] for i in range( 0 , len(text), chunk_size)]\n",
    "    return _\n",
    "\n",
    "pdf_content=[]\n",
    "for page_idx in range(len(pdf.pages)):\n",
    "\n",
    "    # 把每一页的所有文本提炼出来[]\n",
    "    text = pdf.pages[page_idx].extract_text()\n",
    "   \n",
    "    # 每40句组成一个embedding组\n",
    "    for chunk_text in split_text_fixed_size(text,40):\n",
    "        pdf_content.append({\n",
    "            'page':'page_'+str(page_idx + 1),\n",
    "            'content':chunk_text\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a10256",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pdf_content:\n",
    "    print(i[\"page\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a9d9f-b7c7-49ec-9d01-3b2202b8fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "\n",
    "model = SentenceTransformer(\"BAAI/bge-small-zh-v1.5\",device='cuda')\n",
    "\n",
    "## 处理句子分词\n",
    "questions_sentences = [x['question'] for x in questions]\n",
    "\n",
    "# 之前x[\"content\"]是一页的内容，现在是chunk_size为40的小段话，同时也有这段话的页码，也就是一个页码会对应好几段话\n",
    "pdf_content_sentences = [x[\"content\"] for x in pdf_content]\n",
    "\n",
    "# # embedding处理\n",
    "question_embeddings = model.encode(questions_sentences,normalize_embeddings=True)\n",
    "pdf_embeddings = model.encode(pdf_content_sentences,normalize_embeddings=True)\n",
    "\n",
    "len(pdf_content_sentences)\n",
    "len(pdf_embeddings)\n",
    "# 3743个小段，就有3743个向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7774c60-58d0-4baa-a460-7da946bf7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 召回评分\n",
    "for query_idx, feat in enumerate(question_embeddings):\n",
    "    score = feat @ pdf_embeddings.T\n",
    "    max_score_page_idx = score.argsort()[-1] # 之前是idx一一对应page，现在是几个idx同一个page，找page方式自然不同\n",
    "    questions[query_idx]['reference'] = pdf_content[max_score_page_idx]['page']\n",
    "\n",
    "with open('../data/Coggle比赛数据/汽车知识问答/submit_bgesmall_Text_Segment.json', 'w', encoding='utf8') as up:\n",
    "    json.dump(questions, up, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e821f48",
   "metadata": {},
   "source": [
    "## BM25 + BGE/Text Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e7a1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba, json, pdfplumber\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "questions = json.load(open(\"../data/Coggle比赛数据/汽车知识问答/questions.json\",encoding=\"utf8\"))\n",
    "pdf = pdfplumber.open(\"../data/Coggle比赛数据/汽车知识问答/初赛训练数据集.pdf\")\n",
    "pdf_content = []\n",
    "\n",
    "def split_text_fixed_size(text, chunk_size):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "for page_idx in range(len(pdf.pages)):\n",
    "    text = pdf.pages[page_idx].extract_text()\n",
    "    for chunk_text in split_text_fixed_size(text, 40):\n",
    "        pdf_content.append({\n",
    "            'page': 'page_' + str(page_idx + 1),\n",
    "            'content': chunk_text\n",
    "        })\n",
    "\n",
    "pdf_content_words = [jieba.lcut(x['content']) for x in pdf_content]\n",
    "bm25 = BM25Okapi(pdf_content_words)\n",
    "\n",
    "model = SentenceTransformer('BAAI/bge-small-zh-v1.5',device='cuda')\n",
    "question_sentences = [x['question'] for x in questions]\n",
    "pdf_content_sentences = [x['content'] for x in pdf_content]\n",
    "\n",
    "question_embeddings = model.encode(question_sentences, normalize_embeddings=True)\n",
    "pdf_embeddings = model.encode(pdf_content_sentences, normalize_embeddings=True)\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "for query_idx, feat in enumerate(question_embeddings):\n",
    "    score1 = feat @ pdf_embeddings.T\n",
    "    score2 = bm25.get_scores(jieba.lcut(questions[query_idx][\"question\"]))\n",
    "\n",
    "    score = rankdata(score1) + rankdata(score2)\n",
    "    max_score_page_idx = score.argsort()[-1]\n",
    "    questions[query_idx]['reference'] = pdf_content[max_score_page_idx]['page']\n",
    "\n",
    "with open('../data/Coggle比赛数据/汽车知识问答/submit_bm25_bgesmall_Text_Segment.json', 'w', encoding='utf8') as up:\n",
    "    json.dump(questions, up, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c35f68",
   "metadata": {},
   "source": [
    "## BM25 with ReRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0a701b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba, json, pdfplumber\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "questions = json.load(open(\"../data/Coggle比赛数据/汽车知识问答/questions.json\",encoding=\"utf8\"))\n",
    "pdf = pdfplumber.open(\"../data/Coggle比赛数据/汽车知识问答/初赛训练数据集.pdf\")\n",
    "pdf_content = []\n",
    "for page_idx in range(len(pdf.pages)):\n",
    "    pdf_content.append({\n",
    "        'page': 'page_' + str(page_idx + 1),\n",
    "        'content': pdf.pages[page_idx].extract_text()\n",
    "    })\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-base')\n",
    "rerank_model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base')\n",
    "rerank_model.cuda() # 显卡\n",
    "rerank_model.eval() # 推理模式\n",
    "\n",
    "pdf_content_words = [jieba.lcut(x['content']) for x in pdf_content]\n",
    "bm25 = BM25Okapi(pdf_content_words)\n",
    "\n",
    "for query_idx in range(len(questions)):\n",
    "    doc_scores = bm25.get_scores(jieba.lcut(questions[query_idx][\"question\"]))\n",
    "    max_score_page_idxs = doc_scores.argsort()[-3:] # 把每一个问题的前三个最匹配的content的index找出来\n",
    "\n",
    "    # print(query_idx)\n",
    "    # print(doc_scores)\n",
    "    # print(max_score_page_idxs)\n",
    "    pairs = []\n",
    "    for idx in max_score_page_idxs:# 这三个分数最高的放到列表里\n",
    "        pairs.append([questions[query_idx][\"question\"], pdf_content[idx]['content']])\n",
    "    # print(pairs)\n",
    "\n",
    "    # 每一个问题都会得到如下的结果\n",
    "    #[[\"question\",\"content1\"],[\"question\",\"content2\"],[\"question\",\"content3\"]]\n",
    "\n",
    "\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    # print(inputs)\n",
    "    # exit()\n",
    "    with torch.no_grad(): # 推理时不用计算梯度，可以节省内存，提高效率\n",
    "\n",
    "        # 把'input_ids', 'attention_mask'的值放cuda里，各种形式结构未发生变化\n",
    "        inputs = {key: inputs[key].cuda() for key in inputs.keys()}\n",
    "        \n",
    "        # rerank_model进行打分,\n",
    "        # 结果：tensor([-6.4908, -5.9194, -2.6678], device='cuda:0')\n",
    "        scores = rerank_model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "\n",
    "    # 选出分数最高的一项：2\n",
    "    max_score_page_idx = max_score_page_idxs[scores.cpu().numpy().argmax()]\n",
    "    questions[query_idx]['reference'] = 'page_' + str(max_score_page_idx + 1)\n",
    "\n",
    "with open('../data/Coggle比赛数据/汽车知识问答/submit_bm25_rerangk.json', 'w', encoding='utf8') as up:\n",
    "    json.dump(questions, up, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
