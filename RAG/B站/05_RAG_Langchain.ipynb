{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a04176f2",
   "metadata": {},
   "source": [
    "## langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d0fbc4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文本\n",
    "\n",
    "loader = PyMuPDFLoader(\"..\\data\\Coggle比赛数据\\汽车知识问答\\初赛训练数据集.pdf\")\n",
    "PDF_data=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "68c6dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分文本\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=5)\n",
    "all_splits = text_splitter.split_documents(PDF_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a20cd77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\yx140\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\8b3219a92973c328a8e22fadcfa821b5dc75636a\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\yx140\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\8b3219a92973c328a8e22fadcfa821b5dc75636a\\model.safetensors\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file vocab.txt from cache at C:\\Users\\yx140\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\8b3219a92973c328a8e22fadcfa821b5dc75636a\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\yx140\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\8b3219a92973c328a8e22fadcfa821b5dc75636a\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\yx140\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\8b3219a92973c328a8e22fadcfa821b5dc75636a\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\yx140\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2\\snapshots\\8b3219a92973c328a8e22fadcfa821b5dc75636a\\tokenizer_config.json\n",
      "d:\\ProgramData\\miniconda3\\envs\\big_model\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 利用embedding模型建立向量数据库\n",
    "persist_directory=\"db\"\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "model_kwargs={\"device\":\"cuda\"}\n",
    "embedding =HuggingFaceEmbeddings(model_name=model_name,model_kwargs=model_kwargs)\n",
    "vectordb=Chroma.from_documents(documents=all_splits,embedding=embedding,persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57cc0d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:\\Program Projects\\Python Projects\\DB-GPT\\models\\Qwen2-0.5B\\config.json\n",
      "Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"D:\\\\Program Projects\\\\Python Projects\\\\DB-GPT\\\\models\\\\Qwen2-0.5B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading weights file D:\\Program Projects\\Python Projects\\DB-GPT\\models\\Qwen2-0.5B\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at D:\\Program Projects\\Python Projects\\DB-GPT\\models\\Qwen2-0.5B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file D:\\Program Projects\\Python Projects\\DB-GPT\\models\\Qwen2-0.5B\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 2048\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file tokenizer.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file D:\\Program Projects\\Python Projects\\DB-GPT\\models\\Qwen2-0.5B\\config.json\n",
      "Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"D:\\\\Program Projects\\\\Python Projects\\\\DB-GPT\\\\models\\\\Qwen2-0.5B\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading weights file D:\\Program Projects\\Python Projects\\DB-GPT\\models\\Qwen2-0.5B\\model.safetensors\n",
      "Will use torch_dtype=torch.bfloat16 as defined in model's config object\n",
      "Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at D:\\Program Projects\\Python Projects\\DB-GPT\\models\\Qwen2-0.5B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file D:\\Program Projects\\Python Projects\\DB-GPT\\models\\Qwen2-0.5B\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 2048\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "model =AutoModelForCausalLM.from_pretrained('D:\\Program Projects\\Python Projects\\DB-GPT\\models\\Qwen2-0.5B')\n",
    "model=model.cuda()\n",
    "\n",
    "model_path='D:\\Program Projects\\Python Projects\\DB-GPT\\models\\Qwen2-0.5B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype='auto'\n",
    "    ).eval()\n",
    "\n",
    "pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.4\n",
    "    )\n",
    "\n",
    "return_model = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a36723ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=return_model,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = retriever,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba2b2968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\envs\\big_model\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': '汽车有关知识',\n",
       " 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n车辆与前方车辆的距离保持为设定距离、提醒您有碰撞危险等。有些\\n情况下，如必要，驾驶辅助系统还会自主刹车。\\n警告！\\n■驾驶辅助系统对驾驶员来说是一种辅助功能，并不能处理所有情\\n\\n■如果有，请了解这些图标的含义和相应的处理措施，请参见\\n指示灯和警告灯（页码74）。\\n■如果无，转至步骤2。\\n2 检查是否有任何系统功能下降或受限。\\n\\n的穿孔，还可以对轮胎重新充气。\\n说明！\\n□补胎套装仅适用于胎面上存在刺穿的密封轮胎。如果轮胎有较大\\n裂缝、裂痕或类似损坏时，无法使用补胎套装密封轮胎。\\n补胎套装位于后备厢盖板下方。补胎套装配备有：\\n\\n■补胎液罐。\\n■电动充气泵。\\n\\nQuestion: 汽车有关知识\\nHelpful Answer: The vehicle's distance from the driver's side of the vehicle is set to the maximum distance that can be driven safely. Some situations, such as when the driver is in a hurry, may require the use of the driver's side brake. \\n\\nThe driver's side brake system will automatically apply the brakes when the driver's side of the vehicle is in collision. \\n\\nWarning! \\n\\n■ The driver's side brake system is a safety feature, but it cannot handle all situations. \\n\\n■ If there is\"}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"汽车有关知识\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535bbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
