{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据库\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\envs\\test\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[24553, 34439, 6238, 37248, 40120, 32004, 5225, 13258, 225, 43929]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词编码\n",
    "text = \"你好，今天过的怎样\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\") #不同模型的分词编码器不同\n",
    "encoded_text = tokenizer(text)[\"input_ids\"]\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好，今天过的怎样'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词解码\n",
    "decoded_text=tokenizer.decode(encoded_text)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24553, 34439, 6238, 37248, 40120, 32004, 5225, 13258, 225, 43929],\n",
       " [9078, 235, 14274, 39915, 236],\n",
       " [24553, 18617, 97],\n",
       " [35724, 9078, 235, 16677]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 列表编码\n",
    "list_text = [\"你好，今天过的怎样\",\"还不错\",\"你呢\",\"也还行\"]\n",
    "encoded_text = tokenizer(list_text)\n",
    "encoded_text[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24553, 34439, 6238, 37248, 40120, 32004, 5225, 13258, 225, 43929],\n",
       " [9078, 235, 14274, 39915, 236, 0, 0, 0, 0, 0],\n",
       " [24553, 18617, 97, 0, 0, 0, 0, 0, 0, 0],\n",
       " [35724, 9078, 235, 16677, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由于每段文字长度不一致，导致生成的编码长度也不一样，导致我们生成的tensor也不一样\n",
    "# 我们需要使得编码的长度一样，操作相同长度的tensor\n",
    "# 这时需要padding,也就是填充，填充是一种处理不同长度文本编码的方式\n",
    "\n",
    "tokenizer.pad_token=tokenizer.eos_token #指定0词元，作为填充词元，同时也是文本结束的次元\n",
    "encoded_texts_longest=tokenizer(list_text,padding=True)\n",
    "encoded_texts_longest [\"input_ids\"]\n",
    "# 可见，list中所有文本的长度都一致了，长度不对等的都被0填充了，一个编码就是一个词元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24553, 34439, 6238, 37248, 40120],\n",
       " [9078, 235, 14274, 39915, 236],\n",
       " [24553, 18617, 97],\n",
       " [35724, 9078, 235, 16677]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指令的长度是有限的，不能无限填充，除了填充之外，还有个截断操作\n",
    "encoded_texts_truncation= tokenizer(list_text,max_length=5,truncation=True)\n",
    "encoded_texts_truncation[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[32004, 5225, 13258, 225, 43929],\n",
       " [9078, 235, 14274, 39915, 236],\n",
       " [24553, 18617, 97],\n",
       " [35724, 9078, 235, 16677]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 但是往往文本中的重要信息都在右边，所以我们可以选择截断左边，防止重要信息被截断了\n",
    "tokenizer.truncation_side='left'\n",
    "encoded_texts_truncation_left= tokenizer(list_text,max_length=5,truncation=True)\n",
    "encoded_texts_truncation_left[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[32004, 5225, 13258, 225, 43929],\n",
       " [9078, 235, 14274, 39915, 236],\n",
       " [24553, 18617, 97, 0, 0],\n",
       " [35724, 9078, 235, 16677, 0]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实际任务中，会同时开启填充和截断 \n",
    "tokenizer.truncation_side='left'\n",
    "encoded_texts_truncation_both= tokenizer(list_text,max_length=5,padding=True,truncation=True)\n",
    "encoded_texts_truncation_both[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24553, 34439,  6238, 37248, 40120, 32004,  5225, 13258,   225,\n",
       "        43929]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实际使用中会设置分词编码器的最大长度\n",
    "max_length=2048 # 设置最大长度\n",
    "\n",
    "# 最大长度分词\n",
    "tokenizer_inputs=tokenizer(text,return_tensors=\"np\",truncation=True,max_length=max_length)\n",
    "tokenizer_inputs[\"input_ids\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
