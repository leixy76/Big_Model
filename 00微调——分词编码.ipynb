{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据库\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24553, 34439, 6238, 37248, 40120, 32004, 5225, 13258, 225, 43929]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词编码\n",
    "\n",
    "## 创建文本\n",
    "text = \"你好，今天过的怎样\"\n",
    "\n",
    "## 加载分词器（对应模型）\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\") \n",
    "\n",
    "## 分词编码\n",
    "encoded_text = tokenizer(text)[\"input_ids\"]\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好，今天过的怎样'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词解码\n",
    "decoded_text=tokenizer.decode(encoded_text)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24553, 34439, 6238, 37248, 40120, 32004, 5225, 13258, 225, 43929],\n",
       " [9078, 235, 14274, 39915, 236],\n",
       " [24553, 18617, 97],\n",
       " [35724, 9078, 235, 16677]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 列表分词编码\n",
    "list_text = [\"你好，今天过的怎样\",\"还不错\",\"你呢\",\"也还行\"]\n",
    "encoded_text = tokenizer(list_text)\n",
    "encoded_text[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24553, 34439, 6238, 37248, 40120, 32004, 5225, 13258, 225, 43929],\n",
       " [9078, 235, 14274, 39915, 236, 0, 0, 0, 0, 0],\n",
       " [24553, 18617, 97, 0, 0, 0, 0, 0, 0, 0],\n",
       " [35724, 9078, 235, 16677, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文字长度不一致，生成的编码长度也不一致，需要操作相同长度的编码tensor\n",
    "# 需要padding填充，填充是一种处理不同长度文本编码的方式\n",
    "\n",
    "## 指定0，作为填充词元，同时也是文本结束的次元\n",
    "tokenizer.pad_token=tokenizer.eos_token \n",
    "\n",
    "## padding 填充\n",
    "encoded_texts_longest=tokenizer(list_text,padding=True)\n",
    "encoded_texts_longest [\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24553, 34439, 6238, 37248, 40120],\n",
       " [9078, 235, 14274, 39915, 236],\n",
       " [24553, 18617, 97],\n",
       " [35724, 9078, 235, 16677]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指令的长度是有限的，不能无限填充，除了填充之外，还有个截断操作 truncation=True\n",
    "\n",
    "## 设置最大长度，对超过长度的进行截断\n",
    "encoded_texts_truncation= tokenizer(list_text,max_length=5,truncation=True)\n",
    "encoded_texts_truncation[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[32004, 5225, 13258, 225, 43929],\n",
       " [9078, 235, 14274, 39915, 236],\n",
       " [24553, 18617, 97],\n",
       " [35724, 9078, 235, 16677]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 往往文本中的重要信息都在右边，所以我们可以选择截断左边，防止重要信息被截断\n",
    "\n",
    "## 设置左截断\n",
    "tokenizer.truncation_side='left'\n",
    "\n",
    "## 截断\n",
    "encoded_texts_truncation_left= tokenizer(list_text,max_length=5,truncation=True)\n",
    "encoded_texts_truncation_left[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[32004, 5225, 13258, 225, 43929],\n",
       " [9078, 235, 14274, 39915, 236],\n",
       " [24553, 18617, 97, 0, 0],\n",
       " [35724, 9078, 235, 16677, 0]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实际任务中，会同时开启填充和截断 \n",
    "\n",
    "## 设置左截断\n",
    "tokenizer.truncation_side='left'\n",
    "\n",
    "## 开启填充和截断\n",
    "encoded_texts_truncation_both= tokenizer(list_text,max_length=5,padding=True,truncation=True)\n",
    "encoded_texts_truncation_both[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24553, 34439,  6238, 37248, 40120, 32004,  5225, 13258,   225,\n",
       "        43929]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词编码的最大长度是有限的，不能无限输入\n",
    "max_length=2048 # 设置最大长度\n",
    "\n",
    "# 最大长度分词\n",
    "tokenizer_inputs=tokenizer(text,return_tensors=\"np\",truncation=True,max_length=max_length)\n",
    "tokenizer_inputs[\"input_ids\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
