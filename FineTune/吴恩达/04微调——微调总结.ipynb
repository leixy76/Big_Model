{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#总结\n",
    "\n",
    "# 1. 明确你的任务\n",
    "\n",
    "# 2. 收集与任务输入和输出有关的数据，并对你的数据进行组织整理\n",
    "\n",
    "# 3. 数据不够可以使用AI生成、或使用提示词模板来创建数据集\n",
    "\n",
    "# 4. 先微调一个小模型，建议0.4B-1B参数的模型，以了解模型的表现\n",
    "\n",
    "# 5. 可以调整用来对模型进行微调的数据量，观察数据量对模型微调结果的影响\n",
    "\n",
    "# 6. 随后可以评估模型，看看哪里做的好，哪里做的不好\n",
    "\n",
    "# 7. 收集更多的数据，通过评估结果来持续改进模型\n",
    "\n",
    "# 8. 提高任务复杂性，增加模型规模适应复杂任务\n",
    "\n",
    "# 9. 更困难的任务需要更大的模型来处理\n",
    "\n",
    "# 10. 使用更大的模型，推荐一种PEFT的方法，也就是参数高效微调，可以帮助你更高效的使用参数和训练模型的方法\n",
    "## 其中特别推荐LoRA,低秩适应，大大减少了需要训练的参数，\n",
    "## 对于GPT3,发现可将其减少1w倍\n",
    "\n",
    "\n",
    "# 11. LoRA会冻结部分层中，主要的预训练权重，产生新的权重（原始权重变动的秩分解矩阵）\n",
    "# 我们可以分别训练这些权重，与预训练的权重相结合，并在推理时，能将他们合并回主要的预训练权重，\n",
    "# 从而高效的得到微调模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
